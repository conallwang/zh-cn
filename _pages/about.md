---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

æˆ‘ç›®å‰æ˜¯[æ¸…åå¤§å­¦è®¡ç®—æœºç³»](https://www.cs.tsinghua.edu.cn/)çš„ä¸€ååšå£«ç”Ÿï¼Œå¯¼å¸ˆæ˜¯[å¼ æ¾æµ·](https://www.cs.tsinghua.edu.cn/info/1117/3538.htm)å‰¯æ•™æˆã€‚åœ¨è¿™ä¹‹å‰ï¼Œæˆ‘åœ¨[åŒ—äº¬ç§‘æŠ€å¤§å­¦è®¡ç®—æœºä¸é€šä¿¡å·¥ç¨‹å­¦é™¢](https://scce.ustb.edu.cn/)è·å¾—äº†æˆ‘çš„å­¦å£«å­¦ä½ã€‚</br>

æˆ‘çš„ç ”ç©¶å…´è¶£ä¸»è¦èšç„¦åœ¨<span style="color:red">æ•°å­—äºº</span> å’Œ <span style="color:red">è®¡ç®—æœºè§†è§‰</span>é¢†åŸŸï¼ŒåŒ…æ‹¬æ•°å­—äººä½“/äººå¤´çš„åˆ›å»ºå’Œç¼–è¾‘ï¼Œå›¾åƒ/è§†é¢‘çš„ç”Ÿæˆæ¨¡å‹ï¼Œä»¥åŠæ–°çš„3Dè¡¨ç¤ºæ–¹å¼ã€‚</br>

ç°åœ¨ï¼Œæˆ‘æ­£åœ¨æ”»è¯»åšå£«å­¦ä½çš„ç¬¬å››å¹´ï¼ˆæ€»å…±äº”å¹´ï¼‰ï¼Œ<span style="color:red">å°†åœ¨2025å¹´6æœˆä»½æ¯•ä¸šã€‚</span>å¦‚æœæ‚¨æ­£åœ¨å¯»æ‰¾æ•°å­—äººç®—æ³•å·¥ç¨‹å¸ˆï¼Œæ¬¢è¿è”ç³»æˆ‘ï¼ˆwangcong20@mails.tsinghua.edu.cnï¼‰ï¼Œå·¥ä½œåœ°ç‚¹æœ€å¥½åœ¨åŒ—äº¬æˆ–è€…å‘¨è¾¹ã€‚</br>

<span class='anchor' id='news'></span>

# ğŸ”¥ æ–°é—»
- *2023.08*: &nbsp;ğŸ‰ ä¸€ç¯‡è®ºæ–‡è¢« **SIGRRAPH Asia 2023** æ¥å—!
- *2023.07*: &nbsp;ğŸ‰ ä¸€ç¯‡è®ºæ–‡è¢« **ICCV 2023** æ¥å—!
- *2022.07*: &nbsp; ä½œä¸ºæ—¥å¸¸å®ä¹ ç”Ÿå…¥èŒè…¾è®¯AI Lab.
- *2022.02*: &nbsp;ğŸ‰ ä¸€ç¯‡è®ºæ–‡è¢« **ICRA 2022** æ¥å—!


<span class='anchor' id='publications'></span>

# ğŸ“ å‘è¡¨è®ºæ–‡

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">SIGGRAPH Asia 2023</div><img src='images/npva_teaser.jpeg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Neural Point-based Volumetric Avatar: Surface-guided Neural Points for Efficient and Photorealistic Volumetric Head Avatar](https://dl.acm.org/doi/10.1145/3610548.3618204)

**Cong Wang**, Di Kang, Yan-Pei Cao, Linchao Bao, Ying Shan, Song-Hai Zhang

[**Project**](https://conallwang.github.io/npva.github.io/) <strong><span class='show_paper_citations' data='0gSn6sgAAAAJ:9yKSN-GCB0IC'></span></strong>
- NPVA ä½¿ç”¨è¡¨é¢å¼•å¯¼çš„ç¥ç»ç‚¹åœ¨æŒ‘æˆ˜æ€§åŒºåŸŸï¼ˆä¾‹å¦‚å˜´å†…éƒ¨ã€çœ¼ç›å’Œèƒ¡å­ï¼‰è·å¾—äº†æ›´é«˜è´¨é‡çš„æ¸²æŸ“ç»“æœã€‚
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2023</div><img src='images/lolep_teaser.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[LoLep: Single-View View Synthesis with Locally-Learned Planes and Self-Attention Occlusion Inference](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_LoLep_Single-View_View_Synthesis_with_Locally-Learned_Planes_and_Self-Attention_Occlusion_ICCV_2023_paper.pdf)

**Cong Wang**, Yu-Ping Wang, Dinesh Manocha

[**Project**](None) <strong><span class='show_paper_citations' data='0gSn6sgAAAAJ:2osOgNQ5qMEC'></span></strong>
- By regressing Locally-Learned Planes, LoLep is able to generate better novel views from one single RGB image. 
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICRA 2022</div><img src='images/motionhint_pipe.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[MotionHint: Self-Supervised Monocular Visual Odometry with Motion Constraints](https://dl.acm.org/doi/abs/10.1109/ICRA46639.2022.9812288)

**Cong Wang**, Yu-Ping Wang, Dinesh Manocha

[**Project**](https://github.com/conallwang/MotionHint) <strong><span class='show_paper_citations' data='0gSn6sgAAAAJ:u5HHmVD_uO8C'></span></strong>
- MotionHint is able to be easily applied to existing open-sourced state-of-the-art SSM-VO systems to greatly improve the performance (reducing ATE by up to 28.73%).
</div>
</div>

- [ORBBuf: A robust buffering method for remote visual SLAM](https://dl.acm.org/doi/abs/10.1109/IROS51168.2021.9635950), Yu-Ping Wang, Zi-xin Zou, **Cong Wang**, et al. **IROS 2021**


<span class='anchor' id='honors-and-awards'></span>

# ğŸ– è£èª‰å¥–åŠ±
- *2023.10* Hefei Excellence Scholarship
- *2023.09* Longhu Scholarship
- *2023.05* 2023 Tencent AI Lab Rhino-Bird Elite Talent
- *2022.09* Longhu Scholarship
- *2019.11* National Scholarship (1/446)
- *2019.04* the Mathematical Contest in Modeling, Meritorious Winner (Top 4%)
- *2018.11* National Scholarship (1/446)
- *2018.04* the Mathematical Contest in Modeling, Meritorious Winner (Top 4%)
- *2017.11* People's Special Scholarship (1/145)
- *2017.11* "Guan Zhi" Scholarship (1/446)

<span class='anchor' id='educations'></span>

# ğŸ“– æ•™è‚²èƒŒæ™¯
- *2020.09 - 2024.04 (now)*, Ph.D. student, the Department of Computer Science and Technology, Tsinghua University, Beijing.
- *2016.09 - 2020.06*, Undergraduate, the School of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing.

<span class='anchor' id='talks'></span>

# ğŸ’¬ é‚€è¯·æ±‡æŠ¥
- *2023.02*, Paper Sharing, Tencent internal talk. 
- *2022.07*, show my paper, invited by BKUNYUN, [video](https://www.bilibili.com/video/BV1cB4y1C7Zw/?spm_id_from=333.337.search-card.all.click)

<span class='anchor' id='internships'></span>

# ğŸ’» å®ä¹ ç»å†
- *2022.07 - 2024.04 (now)*, Tencent AI Lab, Beijing.
