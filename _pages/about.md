---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

æˆ‘ç›®å‰æ˜¯è…¾è®¯æ··å…ƒéƒ¨é—¨çš„æŠ€æœ¯ç ”ç©¶å‘˜ï¼Œä¸»è¦æ¢ç´¢åœ¨æ•°å­—äººåŸŸä¸‹çš„è§†é¢‘ç”ŸæˆæŠ€æœ¯ã€‚åœ¨æ­¤ä¹‹å‰ï¼Œæˆ‘æ›¾åœ¨è…¾è®¯ AI Lab å®ä¹ è¿‡ä¸¤å¹´ï¼Œå½“æ—¶ä¸»è¦åœ¨æ¢ç´¢ 3D æ•°å­—äººä½“/äººå¤´çš„é‡å»º/ç¼–è¾‘ç›¸å…³æŠ€æœ¯ï¼Œä¸[ã€Œåº·é ”åšå£«ã€](https://scholar.google.com/citations?user=2ztThPwAAAAJ&hl=en)å’Œ[ã€Œæš´æ—è¶…åšå£«ã€](https://linchaobao.github.io/)å…±äº‹ã€‚æˆ‘ä¹‹å‰ä¹Ÿåœ¨èš‚èšç ”ç©¶é™¢çš„äº¤äº’æ™ºèƒ½å®éªŒå®¤å®ä¹ è¿‡ä¸€å¹´ï¼Œä¸[ã€Œç‹ç’‡åšå£«ã€](https://xuanwangvc.github.io/)ä¸€åŒæ¢ç´¢å¦‚ä½•ä»¥ä½æˆæœ¬å®ç°é«˜è´¨é‡ 3D æ•°å­—äººä½“/äººå¤´é‡å»ºã€‚

å…³äºæ•™è‚²ç»å†ï¼Œæˆ‘åœ¨[æ¸…åå¤§å­¦è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯ç³»](https://www.cs.tsinghua.edu.cn/)è·å¾—äº†æˆ‘çš„åšå£«å­¦ä½ï¼Œå¯¼å¸ˆæ˜¯[ã€Œå¼ æ¾æµ·ã€](https://www.cs.tsinghua.edu.cn/info/1117/3538.htm)å‰¯æ•™æˆå’Œ[ã€Œç‹ç‘€å±ã€](https://scholar.google.com/citations?hl=en&user=QqdDO64AAAAJ)å‰¯æ•™æˆã€‚å®éªŒå®¤çš„åç§°æ˜¯ CSCGï¼Œç”±[ã€Œèƒ¡äº‹æ°‘ã€](https://scholar.google.com/citations?user=LDb4tb0AAAAJ&hl=en)é™¢å£«åˆ›ç«‹ã€‚åœ¨è¿™ä¹‹å‰ï¼Œæˆ‘åœ¨[åŒ—äº¬ç§‘æŠ€å¤§å­¦è®¡ç®—æœºä¸é€šä¿¡å·¥ç¨‹å­¦é™¢](https://scce.ustb.edu.cn/)è·å¾—äº†æˆ‘çš„å­¦å£«å­¦ä½ã€‚

ç›®å‰æˆ‘çš„ç ”ç©¶å…´è¶£ä¸»è¦èšç„¦åœ¨<span style="color:red; font-weight: bold;">è§†é¢‘ç”Ÿæˆï¼Œæ•°å­—äººå’Œè®¡ç®—æœºè§†è§‰</span>ï¼ŒåŒ…æ‹¬ 2D/3D æ•°å­—äººåº”ç”¨ï¼Œå›¾åƒ/è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œä»¥åŠSFT/RLHFç­‰åè®­ç»ƒæŠ€æœ¯ã€‚

<span class='anchor' id='news'></span>

# ğŸ”¥ æ–°é—»
- *2025.07*: &nbsp; ä½œä¸ºæŠ€æœ¯ç ”ç©¶å‘˜å…¥èŒ**è…¾è®¯æ··å…ƒ**.
- *2025.07*ï¼š&nbsp;ğŸ‰ 2 ç¯‡è®ºæ–‡è¢« **ICCV 2025** æ¥å—!
- *2025.03*ï¼š&nbsp;ğŸ‰ **MeGA** è¢« **CVPR 2025** æ¥å—!
- *2024.07*: &nbsp; ä½œä¸ºå®ä¹ ç”Ÿå…¥èŒ**èš‚èšç ”ç©¶é™¢**.
- *2023.08*: &nbsp;ğŸ‰ **Neural Point-based Volumetric Avatars** è¢« **SIGRRAPH Asia 2023** æ¥å—!
- *2023.07*: &nbsp;ğŸ‰ **LoLep** è¢« **ICCV 2023** æ¥å—!
- *2022.07*: &nbsp; ä½œä¸ºæ—¥å¸¸å®ä¹ ç”Ÿå…¥èŒ**è…¾è®¯AI Lab**.
- *2022.02*: &nbsp;ğŸ‰ **MotionHint** è¢« **ICRA 2022** æ¥å—!


<span class='anchor' id='publications'></span>

# ğŸ“ å‘è¡¨è®ºæ–‡

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2025</div><img src='images/pipeline.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[SVG-Head: Hybrid Surface-Volumetric Gaussians for High-Fidelity Head Reconstruction and Real-Time Editing](https://arxiv.org/pdf/2508.09597)

He-Yi Sun, **Cong Wang**, Tian-Xing Xu, Jingwei Huang, Di Kang, Chunchao Guo, Song-Hai Zhang

[**Project**](https://heyy-sun.github.io/SVG-Head/) <strong><span class='show_paper_citations' data='0gSn6sgAAAAJ:9yKSN-GCB0IC'></span></strong>
- é€šè¿‡æå‡º surf-GS æ¥å°†å…¨å±€è¡¨é¢å»ºæ¨¡ä¸ºä¸€å¼ æ˜¾å¼çº¹ç†å›¾ï¼Œä»¥åŠæå‡º vol-GS æ¥é«˜ä¿çœŸåœ°æ¸²æŸ“éæœ—åšç‰¹æ€§è¡¨é¢ï¼ŒSVG-Head ä¸ä»…æ”¯æŒé«˜è´¨é‡çš„äººå¤´æ¸²æŸ“ï¼Œè€Œä¸”æ”¯æŒå®æ—¶åœ°ã€ç»†ç²’åº¦åœ°çº¹ç†ç¼–è¾‘åŠŸèƒ½ã€‚
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2025</div><img src='images/mega_teaser.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[MeGA: Hybrid Mesh-Gaussian Head Avatar for High-Fidelity Rendering and Head Editing](https://arxiv.org/abs/2404.19026)

**Cong Wang**, Di Kang, He-Yi Sun, Shen-Han Qian, Zi-Xuan Wang, Linchao Bao, Song-Hai Zhang

[**Project**](https://conallwang.github.io/MeGA_Pages/) <strong><span class='show_paper_citations' data='0gSn6sgAAAAJ:9yKSN-GCB0IC'></span></strong>
- æå‡ºä½¿ç”¨**æ›´åŠ åˆé€‚çš„è¡¨è¾¾æ–¹å¼æ¥è¡¨è¾¾äººå¤´çš„ä¸åŒéƒ¨åˆ†**å’Œç›¸åº”çš„**æ··åˆæ¸²æŸ“æ–¹æ³•**ï¼Œæˆ‘ä»¬åœ¨ä¿è¯**é«˜è´¨é‡** 3D äººå¤´é‡å»ºçš„åŸºç¡€ä¸Šï¼Œ**é¦–æ¬¡æ”¯æŒäººå¤´ç¼–è¾‘**ï¼ˆé¢éƒ¨æ¶‚é¸¦å’Œå‘å‹æ›¿æ¢ï¼‰ã€‚

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">SIGGRAPH Asia 2023</div><img src='images/npva_teaser.jpeg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Neural Point-based Volumetric Avatar: Surface-guided Neural Points for Efficient and Photorealistic Volumetric Head Avatar](https://dl.acm.org/doi/10.1145/3610548.3618204)

**Cong Wang**, Di Kang, Yan-Pei Cao, Linchao Bao, Ying Shan, Song-Hai Zhang

[**Project**](https://conallwang.github.io/npva.github.io/) <strong><span class='show_paper_citations' data='0gSn6sgAAAAJ:9yKSN-GCB0IC'></span></strong>
- æå‡ºæ–°çš„**è¡¨é¢å¼•å¯¼çš„ç¥ç»ç‚¹è¡¨ç¤º**å’Œç›¸åº”çš„**æ¸²æŸ“åŠ é€Ÿæ–¹æ³•**ï¼Œæå¤§åœ°æ”¹å–„äº† 3D æ•°å­—äººå»ºæ¨¡ä¸­**çœ¼ç›å’Œå£è…”å†…éƒ¨**çš„æ¸²æŸ“è´¨é‡ã€‚

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2023</div><img src='images/lolep_teaser.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[LoLep: Single-View View Synthesis with Locally-Learned Planes and Self-Attention Occlusion Inference](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_LoLep_Single-View_View_Synthesis_with_Locally-Learned_Planes_and_Self-Attention_Occlusion_ICCV_2023_paper.pdf)

**Cong Wang**, Yu-Ping Wang, Dinesh Manocha

[**Project**](None) <strong><span class='show_paper_citations' data='0gSn6sgAAAAJ:2osOgNQ5qMEC'></span></strong>
- æå‡º**ä½¿ç”¨ç¥ç»ç½‘ç»œé‡‡æ ·å™¨**ï¼Œä½¿æ¨ç†æ—¶çš„ **MPI å¹³é¢ä½ç½®æ›´åŠ ç¡®å®šä¸”åˆç†**ï¼ŒåŒæ—¶**å¼•å…¥é‡æŠ•å½±æŸå¤±**æ¥ä¿ƒè¿›é‡‡æ ·å™¨çš„å­¦ä¹ ï¼›åˆ©ç”¨ Attention æœºåˆ¶**æå‡ç½‘ç»œå¯¹äºé®æŒ¡éƒ¨åˆ†çš„æ¨ç†èƒ½åŠ›**ã€‚

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICRA 2022</div><img src='images/motionhint_pipe.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[MotionHint: Self-Supervised Monocular Visual Odometry with Motion Constraints](https://dl.acm.org/doi/abs/10.1109/ICRA46639.2022.9812288)

**Cong Wang**, Yu-Ping Wang, Dinesh Manocha

[**Project**](https://github.com/conallwang/MotionHint) <strong><span class='show_paper_citations' data='0gSn6sgAAAAJ:u5HHmVD_uO8C'></span></strong>
- æå‡ºé€šè¿‡é¢„è®­ç»ƒçš„ PPNet **å¼•å…¥ç›¸æœºè½½å…·ï¼ˆä¸»è¦æ˜¯æ— äººé©¾é©¶æ±½è½¦ï¼‰çš„è¿åŠ¨å…ˆéªŒ**ï¼Œä»è€Œå¯ä»¥æ ¹æ®è¿åŠ¨å…ˆéªŒå¯¹ç½‘ç»œé¢„æµ‹çš„ä½å§¿è¿›è¡ŒçŸ«æ­£ï¼Œå¾—åˆ° SOTA çš„ï¼ˆç†è®ºä¸Šæ€»æ˜¯æ›´å¥½çš„ï¼‰ç»“æœã€‚

</div>
</div>

- [ORBBuf: A robust buffering method for remote visual SLAM](https://dl.acm.org/doi/abs/10.1109/IROS51168.2021.9635950), Yu-Ping Wang, Zi-xin Zou, **Cong Wang**, et al. **IROS 2021**


<span class='anchor' id='honors-and-awards'></span>

# ğŸ– è£èª‰å¥–åŠ±
- *2024.11* **åä¸ºå¥–å­¦é‡‘** (20,000å…ƒ)
- *2024.07* **2023è…¾è®¯çŠ€ç‰›é¸Ÿç²¾è‹±äººæ‰è®¡åˆ’ "ä¼˜ç§€å¥–"** 
- *2023.10* æ¸…åå¤§å­¦ç»¼åˆå¥–å­¦é‡‘ (5,000å…ƒ)
- *2023.09* é¾™æ¹–å¥–å­¦é‡‘ (5,000å…ƒ)
- *2023.05* **2023è…¾è®¯çŠ€ç‰›é¸Ÿç²¾è‹±äººæ‰è®¡åˆ’**
- *2022.10* æ¸…åå¤§å­¦ç»¼åˆå¥–å­¦é‡‘ (5,000å…ƒ)
- *2022.09* é¾™æ¹–å¥–å­¦é‡‘ (5,000å…ƒ)
- *2020.06* **åŒ—äº¬å¸‚ä¼˜ç§€æ¯•ä¸šç”Ÿ**ï¼ˆå‰5%ï¼‰
- *2019.11* **å›½å®¶å¥–å­¦é‡‘** (8,000å…ƒ, 1/446)
- *2019.04* ç¾å›½å¤§å­¦ç”Ÿæ•°å­¦å»ºæ¨¡å¤§èµ›, Meritorious Winner (å‰4%)
- *2018.11* **å›½å®¶å¥–å­¦é‡‘** (8,000å…ƒ, 1/446)
- *2018.04* ç¾å›½å¤§å­¦ç”Ÿæ•°å­¦å»ºæ¨¡å¤§èµ›, Meritorious Winner (å‰4%)
- *2017.11* äººæ°‘ç‰¹ç­‰å¥–å­¦é‡‘ (5,000å…ƒ, 1/145)
- *2017.11* å† ä¹‹å¥–å­¦é‡‘ (10,000å…ƒ, 1/446)

<span class='anchor' id='educations'></span>

# ğŸ“– æ•™è‚²èƒŒæ™¯
- *2020.09 - 2025.06*, åšå£«ç”Ÿ, æ¸…åå¤§å­¦è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯ç³», åŒ—äº¬.
- *2016.09 - 2020.06*, æœ¬ç§‘ç”Ÿ, åŒ—äº¬ç§‘æŠ€å¤§å­¦è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯ç³», åŒ—äº¬.

<span class='anchor' id='talks'></span>

# ğŸ’¬ é‚€è¯·æ±‡æŠ¥
- *2023.12*, é’ˆå¯¹è®ºæ–‡"Neural Point-based Volumetric Avatar: Surface-guided Neural Points for Efficient and Photorealistic Volumetric Head Avatar"çš„å£å¤´æŠ¥å‘Š, SIGGRAPH Asia 2023, æ‚‰å°¼ï¼Œæ–°å—å¨å°”å£«å·, æ¾³å¤§åˆ©äºšã€‚
- *2023.11*, å—[ä¸­å›½å›¾åƒå›¾å½¢å­¦æŠ¥](http://www.cjig.cn/jig/ch/index.aspx)é‚€è¯·è®²è§£è®ºæ–‡, [bilibiliè§†é¢‘](https://www.bilibili.com/video/BV1o64y177Ny/?spm_id_from=333.337.search-card.all.click&vd_source=b4eed9deaadbce01a5a20c7c9374a85e)
- *2022.07*, å—[åŒ—é²²äº‘](https://www.bkunyun.com/)å¹³å°é‚€è¯·è®²è§£è®ºæ–‡, [bilibiliè§†é¢‘](https://www.bilibili.com/video/BV1cB4y1C7Zw/?spm_id_from=333.337.search-card.all.click)
- *2022.05*, é’ˆå¯¹è®ºæ–‡"MotionHint: Self-Supervised Monocular Visual Odometry with Motion Constraints"çš„å£å¤´æŠ¥å‘Šï¼ŒICRA 2022, è´¹åŸ, å®¾å¤•æ³•å°¼äºšå·, ç¾å›½ã€‚

<span class='anchor' id='internships'></span>

# ğŸ’» å®ä¹ ç»å†
- *2025.07 - è‡³ä»Š*, è…¾è®¯æ··å…ƒ, åŒ—äº¬.
- *2025.05 - 2025.07*, æ·˜å¤©é›†å›¢, æ­å·.
- *2024.07 - 2025.05*, èš‚èšç ”ç©¶é™¢, æ­å·.
- *2022.07 - 2024.07*, è…¾è®¯ AI Lab, åŒ—äº¬.
